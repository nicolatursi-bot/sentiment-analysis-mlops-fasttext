name: Sentiment Analysis MLOps - CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  
  # JOB 1: CODE QUALITY
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black
    
    - name: Lint with flake8
      run: |
        flake8 src tests --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src tests --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Check code formatting with black
      run: |
        black --check src tests || true

  # JOB 2: UNIT TESTS
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: code-quality
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov
    
    - name: Run unit tests
      run: |
        python -m pytest tests/ -v --tb=short
    
    - name: Generate coverage report
      run: |
        pytest tests/ --cov=src --cov-report=xml --cov-report=html
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  # JOB 3: INTEGRATION TESTS
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Test data loading pipeline
      run: |
        python -c "
        from src.data_loader_sentiment import SentimentDataLoader
        loader = SentimentDataLoader()
        print('âœ… Data loader initialized successfully')
        print('âœ… Label mapping:', loader.get_label_info())
        "
    
    - name: Test model pipeline
      run: |
        python -c "
        from src.sentiment_model_roberta import RoBERTaSentimentModel
        model = RoBERTaSentimentModel()
        print('âœ… RoBERTa model initialized successfully')
        print('âœ… Model info available')
        "
    
    - name: Test monitoring pipeline
      run: |
        python -c "
        import numpy as np
        from src.monitor import ModelMonitor
        ref_pred = np.random.choice([0, 1, 2], size=50)
        ref_labels = np.random.choice([0, 1, 2], size=50)
        monitor = ModelMonitor('test', ref_pred, ref_labels)
        print('âœ… Monitor initialized successfully')
        print('âœ… All integration tests passed')
        "

  # JOB 4: MODEL VALIDATION
  model-validation:
    name: Model Validation
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Validate RoBERTa model output format
      run: |
        python -c "
        from src.sentiment_model_roberta import SentimentAnalyzer
        
        # Test inference
        analyzer = SentimentAnalyzer()
        test_texts = ['This is great', 'It is okay', 'This is bad']
        result = analyzer.analyze(test_texts)
        
        # Validate format
        assert 'total_texts' in result
        assert 'sentiment_distribution' in result
        assert 'average_confidence' in result
        
        # Validate predictions
        for pred in result['predictions']:
            assert 'label' in pred
            assert 'confidence' in pred
            assert pred['label'] in ['positive', 'neutral', 'negative']
        
        print('âœ… RoBERTa model output format validated')
        print('âœ… All predictions in correct format')
        "

  # JOB 5: DOCUMENTATION CHECK
  documentation:
    name: Documentation Check
    runs-on: ubuntu-latest
    needs: model-validation
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Check README exists
      run: |
        if [ -f "README.md" ]; then
          echo "âœ… README.md found"
          wc -l README.md
        else
          echo "âŒ README.md not found"
          exit 1
        fi
    
    - name: Check docstrings
      run: |
        python -c "
        import os
        import ast
        
        src_dir = 'src'
        missing_docs = []
        
        for file in os.listdir(src_dir):
            if file.endswith('.py') and file != '__init__.py':
                filepath = os.path.join(src_dir, file)
                with open(filepath, 'r') as f:
                    try:
                        tree = ast.parse(f.read())
                        for node in ast.walk(tree):
                            if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                                if not ast.get_docstring(node):
                                    missing_docs.append(f'{file}: {node.name}')
                    except:
                        pass
        
        if missing_docs:
          print('âš ï¸ Found functions/classes without docstrings:')
          for doc in missing_docs:
            print(f'  - {doc}')
        else:
          print('âœ… All functions have docstrings')
        "
    
    - name: Set up Python for config check
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Validate configuration files
      run: |
        python -c "
        import json
        
        # Check config.json
        with open('config.json', 'r') as f:
          config = json.load(f)
          assert 'project_name' in config
          assert 'model' in config
          assert 'monitoring' in config
          assert config['model']['type'] == 'RoBERTa'
          print('âœ… config.json is valid')
        
        # Check requirements.txt
        with open('requirements.txt', 'r') as f:
          lines = f.readlines()
          assert len(lines) > 0
          assert any('transformers' in line for line in lines)
          print(f'âœ… requirements.txt has {len(lines)} packages')
          print('âœ… RoBERTa dependencies detected')
        "

  # JOB 6: PERFORMANCE BENCHMARK
  performance:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    needs: documentation
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run RoBERTa inference benchmark
      run: |
        python -c "
        import time
        from src.sentiment_model_roberta import RoBERTaSentimentModel
        
        # Initialize model
        model = RoBERTaSentimentModel()
        
        # Test texts
        test_texts = ['This product is amazing'] * 10
        
        # Benchmark inference
        start_time = time.time()
        predictions = model.inference(test_texts)
        infer_time = time.time() - start_time
        
        avg_latency = (infer_time * 1000) / len(test_texts)
        
        print(f'Inference latency: {avg_latency:.2f}ms per sample')
        print(f'Batch inference time: {infer_time:.3f}s for {len(test_texts)} samples')
        
        # Validate performance
        assert avg_latency < 500, f'Latency too high: {avg_latency}ms'
        assert len(predictions) == len(test_texts)
        
        print('âœ… Performance benchmarks passed')
        print('âœ… RoBERTa is production-ready')
        "

  # JOB 7: DEPLOYMENT (SUCCESS CHECK)
  deployment:
    name: Deployment Readiness
    runs-on: ubuntu-latest
    needs: performance
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Validate deployment readiness
      run: |
        python -c "
        print('=' * 60)
        print('DEPLOYMENT READINESS CHECK')
        print('=' * 60)
        
        # Check all modules importable
        try:
            from src.data_loader_sentiment import SentimentDataLoader
            print('âœ… data_loader_sentiment imports successful')
        except Exception as e:
            print(f'âŒ data_loader_sentiment import failed: {e}')
            exit(1)
        
        try:
            from src.sentiment_model_roberta import RoBERTaSentimentModel, SentimentAnalyzer
            print('âœ… sentiment_model_roberta imports successful')
        except Exception as e:
            print(f'âŒ sentiment_model_roberta import failed: {e}')
            exit(1)
        
        try:
            from src.monitor import ModelMonitor, PerformanceMetrics, DriftDetector
            print('âœ… monitor imports successful')
        except Exception as e:
            print(f'âŒ monitor import failed: {e}')
            exit(1)
        
        try:
            from src.utils import ConfigManager, setup_logging, get_project_dirs
            print('âœ… utils imports successful')
        except Exception as e:
            print(f'âŒ utils import failed: {e}')
            exit(1)
        
        print()
        print('=' * 60)
        print('ALL SYSTEMS GO - READY FOR DEPLOYMENT âœ…')
        print('=' * 60)
        print()
        print('FASE 1: Implementazione Modello RoBERTa âœ…')
        print('FASE 2: Pipeline CI/CD âœ…')
        print('FASE 3: Monitoraggio Continuo âœ…')
        print()
        print('Status: Production Ready ðŸš€')
        print('=' * 60)
        "
    
    - name: Generate summary
      run: |
        echo "## âœ… Pipeline Status: SUCCESS" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Jobs Completed:" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… Code Quality" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… Unit Tests" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… Integration Tests" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… Model Validation" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… Documentation" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… Performance Benchmark" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… Deployment Readiness" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### All Phases:" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… FASE 1: RoBERTa Model Implementation" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… FASE 2: CI/CD Pipeline" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… FASE 3: Continuous Monitoring" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Status: Production Ready" >> $GITHUB_STEP_SUMMARY